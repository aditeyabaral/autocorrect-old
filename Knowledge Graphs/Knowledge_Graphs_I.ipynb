{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Knowledge Graphs I",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONTbmu9LWh1eY+T925wCm4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditeyabaral/AutoCorrect/blob/master/Knowledge%20Graphs/Knowledge_Graphs_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WykI-tkj2YGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsbUm9mVFPrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install https://github.com/huggingface/neuralcoref-models/releases/download/en_coref_lg-3.0.0/en_coref_lg-3.0.0.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyO-rSui9fY6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6173260e-139b-4f8b-9216-3cb23baa6b3a"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr5Nd0ED2AZ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "2f4359e8-50d5-4bd2-ba89-c39e70a4cdc2"
      },
      "source": [
        "import spacy\n",
        "import neuralcoref\n",
        "import en_core_web_lg\n",
        "import re\n",
        "import pandas as pd\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "nlp = en_core_web_lg.load()\n",
        "neuralcoref.add_to_pipe(nlp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 104 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
            "  return f(*args, **kwds)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x7f6661bb2438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxEpIaCz3jlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = '''Machine learning is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2clt9jZC40ru",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d14b668f-7279-4cde-b869-0bd4406a7af8"
      },
      "source": [
        "'''document = nlp(corpus)\n",
        "for tok in document[:5]:\n",
        "  print(tok.text, tok.dep_)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'document = nlp(corpus)\\nfor tok in document[:5]:\\n  print(tok.text, tok.dep_)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p87uHyaq6X97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_entities(sent):\n",
        "  ent1 = \"\"\n",
        "  ent2 = \"\"\n",
        "\n",
        "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
        "  prv_tok_text = \"\"   # previous token in the sentence\n",
        "\n",
        "  prefix = \"\"\n",
        "  modifier = \"\"\n",
        "  \n",
        "  for tok in nlp(sent):\n",
        "    # if token is a punctuation mark then move on to the next token\n",
        "    if tok.dep_ != \"punct\":\n",
        "      # check: token is a compound word or not\n",
        "      if tok.dep_ == \"compound\":\n",
        "        prefix = tok.text\n",
        "        # if the previous word was also a 'compound' then add the current word to it\n",
        "        if prv_tok_dep == \"compound\":\n",
        "          prefix = prv_tok_text + \" \"+ tok.text\n",
        "      \n",
        "      # check: token is a modifier or not\n",
        "      if tok.dep_.endswith(\"mod\") == True:\n",
        "        modifier = tok.text\n",
        "        # if the previous word was also a 'compound' then add the current word to it\n",
        "        if prv_tok_dep == \"compound\":\n",
        "          modifier = prv_tok_text + \" \"+ tok.text\n",
        "      \n",
        "      if tok.dep_.find(\"subj\") == True:\n",
        "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
        "        prefix = \"\"\n",
        "        modifier = \"\"\n",
        "        prv_tok_dep = \"\"\n",
        "        prv_tok_text = \"\"      \n",
        "\n",
        "      if tok.dep_.find(\"obj\") == True:\n",
        "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
        "         \n",
        "      # update variables\n",
        "      prv_tok_dep = tok.dep_\n",
        "      prv_tok_text = tok.text\n",
        "\n",
        "  return [ent1.strip(), ent2.strip()]\n",
        "\n",
        "def get_relation(sent):\n",
        "\n",
        "  doc = nlp(sent)\n",
        "\n",
        "  # Matcher class object \n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  #define the pattern \n",
        "  pattern = [{'DEP':'ROOT'}, \n",
        "            {'DEP':'prep','OP':\"?\"},\n",
        "            {'DEP':'agent','OP':\"?\"},  \n",
        "            {'POS':'ADJ','OP':\"?\"}] \n",
        "\n",
        "  matcher.add(\"matching_1\", None, pattern) \n",
        "\n",
        "  matches = matcher(doc)\n",
        "  k = len(matches) - 1\n",
        "\n",
        "  span = doc[matches[k][1]:matches[k][2]] \n",
        "\n",
        "  return(span.text)\n",
        "\n",
        "def processSubjectObjectPairs(tokens):\n",
        "    subject = ''\n",
        "    object = ''\n",
        "    relation = ''\n",
        "    subjectConstruction = ''\n",
        "    objectConstruction = ''\n",
        "    for token in tokens:\n",
        "        printToken(token)\n",
        "        if \"punct\" in token.dep_:\n",
        "            continue\n",
        "        if isRelationCandidate(token):\n",
        "            relation = appendChunk(relation, token.lemma_)\n",
        "        if isConstructionCandidate(token):\n",
        "            if subjectConstruction:\n",
        "                subjectConstruction = appendChunk(subjectConstruction, token.text)\n",
        "            if objectConstruction:\n",
        "                objectConstruction = appendChunk(objectConstruction, token.text)\n",
        "        if \"subj\" in token.dep_:\n",
        "            subject = appendChunk(subject, token.text)\n",
        "            subject = appendChunk(subjectConstruction, subject)\n",
        "            subjectConstruction = ''\n",
        "        if \"obj\" in token.dep_:\n",
        "            object = appendChunk(object, token.text)\n",
        "            object = appendChunk(objectConstruction, object)\n",
        "            objectConstruction = ''\n",
        "\n",
        "    return (subject.strip(), relation.strip(), object.strip())\n",
        "\n",
        "def printGraph(triples):\n",
        "    G = nx.Graph()\n",
        "    for triple in triples:\n",
        "        G.add_node(triple[0])\n",
        "        G.add_node(triple[1])\n",
        "        G.add_node(triple[2])\n",
        "        G.add_edge(triple[0], triple[1])\n",
        "        G.add_edge(triple[1], triple[2])\n",
        "\n",
        "    pos = nx.spring_layout(G)\n",
        "    plt.figure()\n",
        "    nx.draw(G, pos, edge_color='black', width=1, linewidths=1,\n",
        "            node_size=500, node_color='seagreen', alpha=0.9,\n",
        "            labels={node: node for node in G.nodes()})\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def entity_pairs(text, coref=True):\n",
        "    text = re.sub(r'\\n+', '.', text)  # replace multiple newlines with period\n",
        "    text = re.sub(r'\\[\\d+\\]', ' ', text)  # remove reference numbers\n",
        "    text = nlp(text)\n",
        "    if coref:\n",
        "        text = nlp(text._.coref_resolved)  # resolve coreference clusters\n",
        "    sentences = [sent.string.strip() for sent in text.sents]  # split text into sentences\n",
        "    ent_pairs = list()\n",
        "    for sent in sentences:\n",
        "        sent = nlp(sent)\n",
        "        spans = list(sent.ents) + list(sent.noun_chunks)  # collect nodes\n",
        "        spans = spacy.util.filter_spans(spans)\n",
        "        with sent.retokenize() as retokenizer:\n",
        "            [retokenizer.merge(span) for span in spans]\n",
        "        dep = [token.dep_ for token in sent]\n",
        "        if (dep.count('obj')+dep.count('dobj'))==1 \\\n",
        "                and (dep.count('subj')+dep.count('nsubj'))==1:\n",
        "            for token in sent:\n",
        "                if token.dep_ in ('obj', 'dobj'):  # identify object nodes\n",
        "                    subject = [w for w in token.head.lefts if w.dep_\n",
        "                               in ('subj', 'nsubj')]  # identify subject nodes\n",
        "                    if subject:\n",
        "                        subject = subject[0]\n",
        "                        # identify relationship by root dependency\n",
        "                        relation = [w for w in token.ancestors if w.dep_ == 'ROOT']  \n",
        "                        if relation:\n",
        "                            relation = relation[0]\n",
        "                            # add adposition or particle to relationship\n",
        "                            if relation.nbor(1).pos_ in ('ADP', 'PART'):  \n",
        "                                relation = ' '.join((str(relation),\n",
        "                                        str(relation.nbor(1))))\n",
        "                        else:\n",
        "                            relation = 'unknown'\n",
        "                        subject, subject_type = refine_ent(subject, sent)\n",
        "                        token, object_type = refine_ent(token, sent)\n",
        "                        ent_pairs.append([str(subject), str(relation), str(token),\n",
        "                                str(subject_type), str(object_type)])\n",
        "    filtered_ent_pairs = [sublist for sublist in ent_pairs\n",
        "                          if not any(str(x) == '' for x in sublist)]\n",
        "    pairs = pd.DataFrame(filtered_ent_pairs, columns=['subject',\n",
        "                         'relation', 'object', 'subject_type',\n",
        "                         'object_type'])\n",
        "    print('Entity pairs extracted:', str(len(filtered_ent_pairs)))\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def refine_ent(ent, sent):\n",
        "    unwanted_tokens = (\n",
        "        'PRON',  # pronouns\n",
        "        'PART',  # particle\n",
        "        'DET',  # determiner\n",
        "        'SCONJ',  # subordinating conjunction\n",
        "        'PUNCT',  # punctuation\n",
        "        'SYM',  # symbol\n",
        "        'X',  # other\n",
        "        )\n",
        "    ent_type = ent.ent_type_  # get entity type\n",
        "    if ent_type == '':\n",
        "        ent_type = 'NOUN_CHUNK'\n",
        "        ent = ' '.join(str(t.text) for t in\n",
        "                nlp(str(ent)) if t.pos_\n",
        "                not in unwanted_tokens and t.is_stop == False)\n",
        "    elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
        "        t = ''\n",
        "        for i in range(len(sent) - ent.i):\n",
        "            if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
        "                t += ' ' + str(ent.nbor(i))\n",
        "            else:\n",
        "                ent = t.strip()\n",
        "                break\n",
        "    return ent, ent_type"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6Nkb0wq8l9x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "f64707c3-af60-4a53-875e-d328c33d531e"
      },
      "source": [
        "'''entity_pairs = [get_entities(i) for i in (nltk.tokenize.sent_tokenize(corpus))]\n",
        "relations = [get_relation(i) for i in (nltk.tokenize.sent_tokenize(corpus))]\n",
        "source = [i[0] for i in entity_pairs]\n",
        "target = [i[1] for i in entity_pairs]\n",
        "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
        "G = nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
        "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
        "plt.show()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'entity_pairs = [get_entities(i) for i in (nltk.tokenize.sent_tokenize(corpus))]\\nrelations = [get_relation(i) for i in (nltk.tokenize.sent_tokenize(corpus))]\\nsource = [i[0] for i in entity_pairs]\\ntarget = [i[1] for i in entity_pairs]\\nkg_df = pd.DataFrame({\\'source\\':source, \\'target\\':target, \\'edge\\':relations})\\nG = nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \\n                          edge_attr=True, create_using=nx.MultiDiGraph())\\npos = nx.spring_layout(G)\\nnx.draw(G, with_labels=True, node_color=\\'skyblue\\', edge_cmap=plt.cm.Blues, pos = pos)\\nplt.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekrMrsKA8qCx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "50db212b-8c3d-4859-f9d4-398cb322506a"
      },
      "source": [
        "pairs = entity_pairs(corpus, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3cbf5aeb9d58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentity_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQV9mVZ-PmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLjraWvLFeTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document._.coref_clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "108JhMiaG_4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}