{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Correction\n",
    "This module is to create a model from a key answer and use it to score other answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "import pytextrank\n",
    "from rake_nltk import Rake\n",
    "import re, string\n",
    "import en_core_web_lg\n",
    "import time\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising preprocessing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopw = set(stopwords.words('english'))\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "sp = en_core_web_lg.load()\n",
    "r = Rake()\n",
    "textrank = pytextrank.TextRank()\n",
    "sp.add_pipe(textrank.PipelineComponent, name=\"textrank\", last=True)\n",
    "model = KeyedVectors.load_word2vec_format(\"GoogleModel.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\", \n",
    "    \"can't've\": \"cannot have\", \n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "    \"hadn't've\": \"had not have\", \n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\", \n",
    "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "    \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "    \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "    \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "    \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "       \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "       \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "       \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "       \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "       \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    temp = \"\"\n",
    "    for i in text.split():\n",
    "        try:\n",
    "            temp+=contraction[i]+' '\n",
    "        except:\n",
    "            temp+= i+' '\n",
    "    text = temp.strip()\n",
    "    text = text.lower().translate(remove_punctuation_map)\n",
    "    text = re.sub(\"[^a-zA-Z#]\",\" \",text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"!\", \"!\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \":\", text)\n",
    "    text = re.sub(r' +',' ',text)\n",
    "    return text.strip()\n",
    "\n",
    "def stopwordremoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [i for i in text if i not in stopw]\n",
    "    return \" \".join(text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "What is machine learning?\n",
    "### Expected Answer : \n",
    "Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternate\n",
    "Machine learning is defined as a computer system that performs a specific task without using explicit instructions. Artificial intelligence is a parent of machine learning. We use training data to build a mathematical model using machine learning algorithms. Machine learning algorithms are used in a variety of applications, such as, email filtering and computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ans.txt\", \"r\") as f:\n",
    "    test_ans = f.read().strip()\n",
    "key_ml = '''Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.'''\n",
    "key_mit = '''The mitochondrion is an organelle found in large numbers in most cells, in which the biochemical processes of respiration and energy production occur. It has a double membrane, the inner part being folded inwards to form cristae.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_set(text, op):\n",
    "    key_tokenized_sentences = sent_tokenize(text)\n",
    "    key_tokenized_words = word_tokenize(text)\n",
    "    if op == \"token_sent\":\n",
    "        return key_tokenized_sentences\n",
    "    elif op == \"token_word\":\n",
    "        return key_tokenized_words\n",
    "    elif op == \"clean_sent\":\n",
    "        return [clean(i) for i in key_tokenized_sentences]\n",
    "    elif op == \"clean_word\":\n",
    "        return [clean(i) for i in key_tokenized_words]\n",
    "    elif op == \"lem_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([lemmatizer.lemmatize(j) for j in i.split()]) for i in key_clean_sentences]\n",
    "    elif op == \"lem_word\":\n",
    "        key_clean_words = pp_set(text, \"clean_word\")\n",
    "        return [lemmatizer.lemmatize(i) for i in key_clean_words]\n",
    "    elif op == \"prep_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([i for i in j.split() if i not in stopw]) for j in key_clean_sentences]\n",
    "    elif op == \"prep_word\":\n",
    "        key_preprocessed_sentences = pp_set(text, \"prep_sent\")\n",
    "        key_preprocessed_words = []\n",
    "        for i in key_preprocessed_sentences:\n",
    "            key_preprocessed_words.extend(word_tokenize(i))\n",
    "        return key_preprocessed_words\n",
    "    elif op == \"pp_lem_word\":\n",
    "        return [lemmatizer.lemmatize(i) for i in pp_set(text, \"prep_word\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features, index2word_set):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if nwords>0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_sim(key1, key2):\n",
    "    vec1 = avg_sentence_vector(pp_set(key1,\"pp_lem_word\"), model, 300, model.index2word)\n",
    "    vec2 = avg_sentence_vector(pp_set(key2,\"pp_lem_word\"), model, 300, model.index2word)\n",
    "    sim1 = cosine_similarity(vec1, vec2)[0][0]\n",
    "#     sim2 = 1/(1+model.wmdistance(key1, key2))\n",
    "#     print(sim1, sim2)\n",
    "    return sim1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    r.extract_keywords_from_sentences(pp_set(text, \"lem_sent\")) # Effectiveness : tokenized > lemmatized > clean \n",
    "    rake_keywords = r.get_ranked_phrases()\n",
    "    spdoc = sp(text)\n",
    "    ner_keywords = []\n",
    "    for ent in spdoc.ents:\n",
    "        ner_keywords.append(ent.text)\n",
    "    spdoc = sp(\" \".join(pp_set(text, \"clean_word\")))\n",
    "    pytr_keywords = []\n",
    "    for p in spdoc._.phrases:\n",
    "        for term in p.chunks:\n",
    "            if term.text not in pytr_keywords and term.text not in stopw:\n",
    "                x = term.text\n",
    "                pytr_keywords.append(x)\n",
    "                \n",
    "    all_keywords = rake_keywords+pytr_keywords+ner_keywords\n",
    "    all_keywords = list(set(all_keywords))\n",
    "    sorted_keywords = list(all_keywords)\n",
    "    sorted_keywords.sort()\n",
    "    for i in range(len(sorted_keywords)):\n",
    "        sorted_keywords[i] = re.sub(r' +',' ',sorted_keywords[i])\n",
    "    \n",
    "    return sorted_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using spaCy Noun Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spdoc = sp(\" \".join(key_lemmatized_sentences))\n",
    "# nounchunk_keywords = list(set([i.text for i in set(spdoc.noun_chunks) if i.text not in stopw and len(min(i.text.split(),key = len))>1]))\n",
    "#nounchunk_keywords = list(set([i.text for i in set(spdoc.noun_chunks) if i.text not in stopw]))\n",
    "#nounchunk_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using N-Grams and Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams = list(ngrams(key_clean_words,2))\n",
    "# trigrams = list(ngrams(key_clean_words,3))\n",
    "# quadgrams = list(ngrams(key_clean_words,4))\n",
    "# pentagrams = list(ngrams(key_clean_words,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quadgrams = list(set([\" \".join([j for j in i if j not in stopw]).strip() for i in quadgrams]))\n",
    "# quadgrams = [i for i in quadgrams if i not in stopw and i!='' and len(i.split())>1]\n",
    "#print(quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_vector_key = dict()\n",
    "# for i in quadgrams:\n",
    "#     ngram_vector_key[i] = [0 for i in range(len(key_clean_sentences))]\n",
    "# for i in range(len(key_clean_sentences)):\n",
    "#     for phrase in ngram_vector_key:\n",
    "#         ngram_vector_key[phrase][i] = (key_clean_sentences[i].count(phrase)/len(word_tokenize(key_clean_sentences[i])))\n",
    "#         df = 0\n",
    "#         for j in key_clean_sentences:\n",
    "#             if phrase in j:\n",
    "#                 df+=1\n",
    "#         ngram_vector_key[phrase][i]*=(1+log((len(key_clean_sentences)+1)/(df+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram_keywords = sorted(ngram_vector_key,key = lambda x:sum(ngram_vector_key[x]),reverse = True)\n",
    "#ngram_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(sorted_keywords):\n",
    "    grouped_keys = []\n",
    "    for i in sorted_keywords:\n",
    "        if len(grouped_keys)==0:\n",
    "            grouped_keys.append([i])\n",
    "            continue\n",
    "        else:\n",
    "            flag = False\n",
    "            for j in grouped_keys:\n",
    "                if i in j:\n",
    "                    flag = True\n",
    "                    break\n",
    "                temp1 = \" \".join([lemmatizer.lemmatize(t) for t in stopwordremoval(i).split()])\n",
    "                for k in j:\n",
    "                    temp2 = \" \".join([lemmatizer.lemmatize(t) for t in stopwordremoval(k).split()])\n",
    "                    short = min(temp1,temp2)\n",
    "                    long = max(temp1,temp2)\n",
    "                    if short in long:\n",
    "                        flag = True\n",
    "                        j.append(i)\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    break            \n",
    "            if flag==False:\n",
    "                grouped_keys.append([i])\n",
    "    temp = []\n",
    "    for i in grouped_keys:\n",
    "        k = sorted(i,key = len)\n",
    "        temp.append(k)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['algorithm',\n",
       "  'conventional algorithm',\n",
       "  'a conventional algorithm',\n",
       "  'machine learning algorithm',\n",
       "  'machine learning algorithms',\n",
       "  'machine learning algorithm build',\n",
       "  'algorithms and statistical models'],\n",
       " ['a mathematical model', 'mathematical model based'],\n",
       " ['a specific task',\n",
       "  'specific task without using explicit instruction relying'],\n",
       " ['subset', 'a subset'],\n",
       " ['wide variety', 'a wide variety'],\n",
       " ['application', 'applications'],\n",
       " ['artificial intelligence'],\n",
       " ['computer system use'],\n",
       " ['computer vision'],\n",
       " ['decisions', 'decision without'],\n",
       " ['develop'],\n",
       " ['difficult'],\n",
       " ['effectively performing'],\n",
       " ['email filtering'],\n",
       " ['explicit instructions'],\n",
       " ['explicitly programmed'],\n",
       " ['infeasible'],\n",
       " ['inference', 'inference instead'],\n",
       " ['machine learning ml'],\n",
       " ['make prediction'],\n",
       " ['order'],\n",
       " ['pattern', 'patterns'],\n",
       " ['perform'],\n",
       " ['predictions'],\n",
       " ['sample data', 'sample data known'],\n",
       " ['scientific study', 'the scientific study'],\n",
       " ['seen'],\n",
       " ['statistical model'],\n",
       " ['task', 'the task'],\n",
       " ['training data'],\n",
       " ['used']]"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_keys = group(extract_keywords(key_ml))\n",
    "grouped_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicates - longer match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(grouped_keys):    \n",
    "    for i in range(len(grouped_keys)):\n",
    "        grouped_keys[i] = list(set(grouped_keys[i]))\n",
    "        temp = list(grouped_keys[i])\n",
    "        process_set = [\" \".join([lemmatizer.lemmatize(l) for l in stopwordremoval(j).split()]) for j in grouped_keys[i]]\n",
    "        process_set = list(set(process_set))\n",
    "        for temp_key1 in grouped_keys[i]:\n",
    "            x = \" \".join([lemmatizer.lemmatize(k) for k in stopwordremoval(temp_key1).split()])\n",
    "            if process_set.count(x)>1:\n",
    "                temp.remove(temp_key1)   \n",
    "        grouped_keys[i] = temp\n",
    "        grouped_keys[i] = sorted(grouped_keys[i])\n",
    "        \n",
    "        \n",
    "    for i in range(len(grouped_keys)):\n",
    "        temp = list(grouped_keys[i])\n",
    "        for j in range(len(grouped_keys[i])):\n",
    "            word = grouped_keys[i][j]\n",
    "            for k in temp:\n",
    "                if word in k and word!=k:\n",
    "                    temp.remove(word)\n",
    "                    break\n",
    "        grouped_keys[i] = sorted(temp,key = len, reverse = True)\n",
    "    grouped_keys = [i for i in grouped_keys if len(i)>0]\n",
    "    return grouped_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_keys = remove_duplicates(grouped_keys)\n",
    "#grouped_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize(grouped_keys):\n",
    "    temp_keywords = []\n",
    "    final_keywords = []\n",
    "    for i in grouped_keys:\n",
    "        for j in i:\n",
    "            temp_keywords.append(j)\n",
    "    \n",
    "    temp_keywords = remove_duplicates(group(temp_keywords))\n",
    "    \n",
    "    for i in temp_keywords:\n",
    "        for j in i:\n",
    "            final_keywords.append(j)\n",
    "    return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keywords = finalize(grouped_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Keywords??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Dictionary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionarize(final_keywords, text):\n",
    "    answer_key = dict()\n",
    "    sentences = pp_set(text, \"token_sent\") \n",
    "#     print(sentences)\n",
    "    for i in sentences:\n",
    "        answer_key[i] = list()\n",
    "    temp = list(final_keywords)\n",
    "    for i in range(len(temp)):\n",
    "        key = \" \".join(pp_set(temp[i], \"token_word\"))\n",
    "#         print(key)\n",
    "        for j in answer_key:\n",
    "            x = j.strip().lower()\n",
    "            if key in x:\n",
    "                answer_key[j].append(key)\n",
    "                final_keywords.remove(temp[i])\n",
    "                break\n",
    "    return answer_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "answer_key = dictionarize(final_keywords, key_ml)\n",
    "#answer_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(answer_key):\n",
    "    vector_keys = []\n",
    "    vector_sent = []\n",
    "    for i in list(answer_key.keys()):\n",
    "        vector_sent.append(avg_sentence_vector(pp_set(i, \"token_word\"), model, 300, model.index2word))\n",
    "        temp = []\n",
    "        for j in list(answer_key[i]):\n",
    "            temp.append(avg_sentence_vector(pp_set(j, \"token_word\"), model, 300, model.index2word))\n",
    "        vector_keys.append(temp)\n",
    "    \n",
    "    return vector_sent, vector_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ans = \"\"\"It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = extract_keywords(test_ans)\n",
    "final_kw = finalize(remove_duplicates(group(kw)))\n",
    "answer_test = dictionarize(final_kw, test_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_final = remove_duplicates(group(final_keywords))\n",
    "new_final = []\n",
    "for i in group_final:\n",
    "    if len(i)>1:\n",
    "        for j in i:\n",
    "            for k in i:\n",
    "                if j!=k and j in k:\n",
    "                    new_final.append(j)\n",
    "    else:\n",
    "        new_final.append(i[-1])\n",
    "#new_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "marks = 0\n",
    "for i in answer_key:\n",
    "    marks+=len(answer_key[i])\n",
    "print(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(key, test):\n",
    "    vec_key_sent, vec_key_keys = vectorize_text(key)\n",
    "    vec_test_sent, vec_test_keys = vectorize_text(test)\n",
    "    sum = 0\n",
    "    sims = dict()\n",
    "    for i in range(len(vec_test_sent)):\n",
    "#         max_sim = -1\n",
    "#         max_pair = tuple()\n",
    "        sims[i] = []\n",
    "        for j in range(len(vec_key_sent)):\n",
    "            sim = cosine_similarity(vec_test_sent[i], vec_key_sent[j])\n",
    "            if sim > 0.7:\n",
    "#                 max_sim = sim\n",
    "#                 max_pair = (i,j)\n",
    "                sims[i].append(j)\n",
    "\n",
    "    #print(sims)\n",
    "    count = 0\n",
    "    for keyidx in sims:\n",
    "        ans_kw = vec_test_keys[keyidx]\n",
    "        key_kw = []\n",
    "        checked = []\n",
    "        for i in sims[keyidx]:\n",
    "            key_kw.extend(vec_key_keys[i])\n",
    "            \n",
    "        for akw in ans_kw:\n",
    "            max_sim = -1\n",
    "            max_kkw = []\n",
    "            for kkw in key_kw:\n",
    "                if kkw in checked:\n",
    "                    continue\n",
    "                sim = cosine_similarity(kkw, akw)[0][0]\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    max_akw = kkw\n",
    "            if sim > 0.9:\n",
    "                sum += 1\n",
    "#                 count += 1\n",
    "            else:\n",
    "                sum += max_sim\n",
    "#                 count += 1\n",
    "            checked.append(max_kkw)\n",
    "    return sum,count                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.76636290550232\n",
      "2/4\n"
     ]
    }
   ],
   "source": [
    "test_score, kw_match = score(answer_key, answer_test)\n",
    "print(test_score)\n",
    "test_score = test_score/marks*4.0\n",
    "# print(kw_match)\n",
    "if (test_score%1)> 0.5:\n",
    "    rem = 1\n",
    "else:\n",
    "    rem = 0\n",
    "final_score = int(test_score)+rem\n",
    "print(final_score,\"/\",4,sep = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.662776231765747"
      ]
     },
     "execution_count": 876,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead.': ['algorithms and statistical models',\n",
       "  'explicit instructions',\n",
       "  'a specific task',\n",
       "  'inference instead',\n",
       "  'patterns',\n",
       "  'perform',\n",
       "  'the scientific study',\n",
       "  'statistical model'],\n",
       " 'It is seen as a subset of artificial intelligence.': ['a subset',\n",
       "  'artificial intelligence',\n",
       "  'seen'],\n",
       " 'Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task.': ['machine learning algorithms',\n",
       "  'mathematical model based',\n",
       "  'a mathematical model',\n",
       "  'decisions',\n",
       "  'explicitly programmed',\n",
       "  'make prediction',\n",
       "  'order',\n",
       "  'predictions',\n",
       "  'the task',\n",
       "  'training data'],\n",
       " 'Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.': ['a conventional algorithm',\n",
       "  'a wide variety',\n",
       "  'applications',\n",
       "  'computer vision',\n",
       "  'develop',\n",
       "  'difficult',\n",
       "  'effectively performing',\n",
       "  'email filtering',\n",
       "  'infeasible',\n",
       "  'used']}"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Machine learning is the scientific study of .': ['the scientific study'],\n",
       " 'F algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions , relying on patterns and reference instead .': ['explicit instructions',\n",
       "  'a specific task',\n",
       "  'f algorithms and statistical models',\n",
       "  'patterns',\n",
       "  'perform',\n",
       "  'reference instead',\n",
       "  'statistical model'],\n",
       " 'It is seen as a subset of astiffrcial intelligence .': ['a subset',\n",
       "  'astiffrcial intelligence',\n",
       "  'seen'],\n",
       " 'Machine leading algorithms build a mathematical model lugt a sample data known as training .': ['a mathematical model lugt',\n",
       "  'sample data known',\n",
       "  'a sample data',\n",
       "  'machine leading algorithms',\n",
       "  'training'],\n",
       " 'Machine learning algorithms ase used in a variety of applications .': ['a variety',\n",
       "  'algorithms ase',\n",
       "  'applications'],\n",
       " 'such as email filtering and computes vision where it is difficult or infeasible to develop a .': ['infeasible',\n",
       "  'computes vision',\n",
       "  'develop',\n",
       "  'difficult',\n",
       "  'email filtering',\n",
       "  'vision'],\n",
       " 'Conventional algosiltm for effectively performing the task is # .': ['effectively performing',\n",
       "  'the task'],\n",
       " 'B 3 #': ['3 #']}"
      ]
     },
     "execution_count": 878,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
