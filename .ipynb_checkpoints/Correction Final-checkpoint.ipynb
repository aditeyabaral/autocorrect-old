{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Correction\n",
    "This module is to create a model from a key answer and use it to score other answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from math import *\n",
    "import time\n",
    "import en_core_web_lg\n",
    "from rake_nltk import Rake\n",
    "import pytextrank\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising preprocessing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopw = set(stopwords.words('english'))\n",
    "remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
    "sp = en_core_web_lg.load()\n",
    "r = Rake()\n",
    "textrank = pytextrank.TextRank()\n",
    "sp.add_pipe(textrank.PipelineComponent, name=\"textrank\", last=True)\n",
    "model1 = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "# model = Doc2Vec.load(\"dtv_semeval_text8_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\",\n",
    "    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\", \"so've\": \"so have\", \"so's\": \"so as\",\n",
    "    \"this's\": \"this is\",\n",
    "    \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    temp = \"\"\n",
    "    for i in text.split():\n",
    "        try:\n",
    "            temp += contraction[i]+' '\n",
    "        except:\n",
    "            temp += i+' '\n",
    "    text = temp.strip()\n",
    "    text = text.lower().translate(remove_punctuation_map)\n",
    "    text = re.sub(\"[^a-zA-Z#]\", \" \", text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"!\", \"!\", text)\n",
    "    text = re.sub(r\"\\/\", \"\", text)\n",
    "    text = re.sub(r\"'\", \"\", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \":\", text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def stopwordremoval(text):\n",
    "    text = word_tokenize(text)\n",
    "    text = [i for i in text if i not in stopw]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "What is machine learning?\n",
    "### Expected Answer : \n",
    "Machine learning is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.\n",
    "### Alternate Student Answer :\n",
    "Machine Learning is the use of mathematical models to enable machines to perform tasks without instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question :\n",
    "What is mitochondrion?\n",
    "### Expected Answer : \n",
    "The mitochondrion is an organelle found in large numbers in most cells, in which the biochemical processes of respiration and energy production occur. It has a double membrane, the inner part being folded inwards to form cristae.\n",
    "### Alternate Student Answer :\n",
    "Mitochondria is the power house of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ans_ml.txt\", \"r\") as f:\n",
    "    test_ans_ml = f.read().strip()\n",
    "with open(\"ans_mit.txt\", \"r\") as f:\n",
    "    test_ans_mit = f.read().strip()\n",
    "key_ml = '''Machine learning (ML) is the scientific study of algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions, relying on patterns and inference instead. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop a conventional algorithm for effectively performing the task.'''\n",
    "key_mit = '''The mitochondrion is an organelle found in large numbers in most cells, in which the biochemical processes of respiration and energy production occur. It has a double membrane, the inner part being folded inwards to form cristae.'''\n",
    "\n",
    "# test_ans_ml = '''Machine Learning is the use of mathematical models to enable machines to perform tasks without instructions.'''\n",
    "# test_ans_mit = '''Mitochondria is the power house of the cell.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5cc590e1a09f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m          for i in sent_tokenize(key_ml)+sent_tokenize(key_mit)]\n\u001b[1;32m      3\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "trtxt = [word_tokenize(i.lower().translate(remove_punctuation_map))\n",
    "         for i in sent_tokenize(key_ml)+sent_tokenize(key_mit)]\n",
    "doc = [TaggedDocument(doc, [i]) for i, doc in enumerate(trtxt)]\n",
    "model.train(doc, total_examples=len(doc), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_set(text, op):\n",
    "    key_tokenized_sentences = sent_tokenize(text)\n",
    "    key_tokenized_words = word_tokenize(text)\n",
    "    if op == \"token_sent\":\n",
    "        return key_tokenized_sentences\n",
    "    elif op == \"token_word\":\n",
    "        return key_tokenized_words\n",
    "    elif op == \"clean_sent\":\n",
    "        return [clean(i) for i in key_tokenized_sentences]\n",
    "    elif op == \"clean_word\":\n",
    "        return [clean(i) for i in key_tokenized_words]\n",
    "    elif op == \"lem_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([lemmatizer.lemmatize(j) for j in i.split()]) for i in key_clean_sentences]\n",
    "    elif op == \"lem_word\":\n",
    "        key_clean_words = pp_set(text, \"clean_word\")\n",
    "        return [lemmatizer.lemmatize(i) for i in key_clean_words]\n",
    "    elif op == \"prep_sent\":\n",
    "        key_clean_sentences = pp_set(text, \"clean_sent\")\n",
    "        return [\" \".join([i for i in j.split() if i not in stopw]) for j in key_clean_sentences]\n",
    "    elif op == \"prep_word\":\n",
    "        key_preprocessed_sentences = pp_set(text, \"prep_sent\")\n",
    "        key_preprocessed_words = []\n",
    "        for i in key_preprocessed_sentences:\n",
    "            key_preprocessed_words.extend(word_tokenize(i))\n",
    "        return key_preprocessed_words\n",
    "    elif op == \"pp_lem_word\":\n",
    "        return [lemmatizer.lemmatize(i) for i in pp_set(text, \"prep_word\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_vector(words, model, num_features, index2word_set):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_sim(key1, key2):\n",
    "    print(\"key1:\", key1)\n",
    "    print(\"key2:\", key2)\n",
    "    try:\n",
    "        sim = model.wv.n_similarity(key1, key2)\n",
    "    except:\n",
    "        vec1 = avg_sentence_vector(\n",
    "            pp_set(key1, \"pp_lem_word\"), model1, 300, model.index2word)\n",
    "        vec2 = avg_sentence_vector(\n",
    "            pp_set(key2, \"pp_lem_word\"), model1, 300, model.index2word)\n",
    "        sim = cosine_similarity(vec1, vec2)[0][0]\n",
    "    finally:\n",
    "        return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    # Effectiveness : tokenized > lemmatized > clean\n",
    "    r.extract_keywords_from_sentences(pp_set(text, \"lem_sent\"))\n",
    "    rake_keywords = r.get_ranked_phrases()\n",
    "    spdoc = sp(text)\n",
    "    ner_keywords = []\n",
    "    for ent in spdoc.ents:\n",
    "        ner_keywords.append(ent.text)\n",
    "    spdoc = sp(\" \".join(pp_set(text, \"clean_word\")))\n",
    "    pytr_keywords = []\n",
    "    for p in spdoc._.phrases:\n",
    "        for term in p.chunks:\n",
    "            if term.text not in pytr_keywords and term.text not in stopw:\n",
    "                x = term.text\n",
    "                pytr_keywords.append(x)\n",
    "\n",
    "    all_keywords = rake_keywords+pytr_keywords+ner_keywords\n",
    "    all_keywords = list(set(all_keywords))\n",
    "    sorted_keywords = list(all_keywords)\n",
    "    sorted_keywords.sort()\n",
    "    for i in range(len(sorted_keywords)):\n",
    "        sorted_keywords[i] = re.sub(r' +', ' ', sorted_keywords[i])\n",
    "\n",
    "    return sorted_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group(sorted_keywords):\n",
    "    grouped_keys = []\n",
    "    for i in sorted_keywords:\n",
    "        if len(grouped_keys) == 0:\n",
    "            grouped_keys.append([i])\n",
    "            continue\n",
    "        else:\n",
    "            flag = False\n",
    "            for j in grouped_keys:\n",
    "                if i in j:\n",
    "                    flag = True\n",
    "                    break\n",
    "                temp1 = \" \".join([lemmatizer.lemmatize(t)\n",
    "                                  for t in stopwordremoval(i).split()])\n",
    "                for k in j:\n",
    "                    temp2 = \" \".join([lemmatizer.lemmatize(t)\n",
    "                                      for t in stopwordremoval(k).split()])\n",
    "                    short = min(temp1, temp2)\n",
    "                    long = max(temp1, temp2)\n",
    "                    if short in long:\n",
    "                        flag = True\n",
    "                        j.append(i)\n",
    "                        break\n",
    "                if flag == True:\n",
    "                    break\n",
    "            if flag == False:\n",
    "                grouped_keys.append([i])\n",
    "    temp = []\n",
    "    for i in grouped_keys:\n",
    "        k = sorted(i, key=len)\n",
    "        temp.append(k)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_keys = group(extract_keywords(key_ml))\n",
    "# grouped_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Duplicates - longer match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(grouped_keys):\n",
    "    for i in range(len(grouped_keys)):\n",
    "        grouped_keys[i] = list(set(grouped_keys[i]))\n",
    "        temp = list(grouped_keys[i])\n",
    "        process_set = [\" \".join([lemmatizer.lemmatize(\n",
    "            l) for l in stopwordremoval(j).split()]) for j in grouped_keys[i]]\n",
    "        process_set = list(set(process_set))\n",
    "        for temp_key1 in grouped_keys[i]:\n",
    "            x = \" \".join([lemmatizer.lemmatize(k)\n",
    "                          for k in stopwordremoval(temp_key1).split()])\n",
    "            if process_set.count(x) > 1:\n",
    "                temp.remove(temp_key1)\n",
    "        grouped_keys[i] = temp\n",
    "        grouped_keys[i] = sorted(grouped_keys[i])\n",
    "\n",
    "    for i in range(len(grouped_keys)):\n",
    "        temp = list(grouped_keys[i])\n",
    "        for j in range(len(grouped_keys[i])):\n",
    "            word = grouped_keys[i][j]\n",
    "            for k in temp:\n",
    "                if word in k and word != k:\n",
    "                    temp.remove(word)\n",
    "                    break\n",
    "        grouped_keys[i] = sorted(temp, key=len, reverse=True)\n",
    "    grouped_keys = [i for i in grouped_keys if len(i) > 0]\n",
    "    return grouped_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_keys = remove_duplicates(grouped_keys)\n",
    "# grouped_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize(grouped_keys):\n",
    "    temp_keywords = []\n",
    "    final_keywords = []\n",
    "    for i in grouped_keys:\n",
    "        for j in i:\n",
    "            temp_keywords.append(j)\n",
    "\n",
    "    temp_keywords = remove_duplicates(group(temp_keywords))\n",
    "\n",
    "    for i in temp_keywords:\n",
    "        for j in i:\n",
    "            final_keywords.append(j)\n",
    "    return final_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_keywords = finalize(grouped_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Dictionary Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionarize(final_keywords, text):\n",
    "    answer_key = dict()\n",
    "    sentences = pp_set(text, \"token_sent\")\n",
    "    for i in sentences:\n",
    "        answer_key[i] = list()\n",
    "    temp = list(final_keywords)\n",
    "    for i in range(len(temp)):\n",
    "        key = \" \".join(pp_set(temp[i], \"token_word\"))\n",
    "        for j in answer_key:\n",
    "            x = j.strip().lower()\n",
    "            if key in x:\n",
    "                answer_key[j].append(key)\n",
    "                final_keywords.remove(temp[i])\n",
    "                break\n",
    "    return answer_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "answer_key = dictionarize(final_keywords, key_ml)\n",
    "# answer_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(answer_key):\n",
    "    vector_keys = []\n",
    "    vector_sent = []\n",
    "    for i in list(answer_key.keys()):\n",
    "        vector_sent.append(avg_sentence_vector(\n",
    "            pp_set(i, \"token_word\"), model1, 300, model1.index2word))\n",
    "        temp = []\n",
    "        for j in list(answer_key[i]):\n",
    "            temp.append(avg_sentence_vector(\n",
    "                pp_set(j, \"token_word\"), model1, 300, model1.index2word))\n",
    "        vector_keys.append(temp)\n",
    "\n",
    "    return vector_sent, vector_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = extract_keywords(test_ans_ml)\n",
    "final_kw = finalize(remove_duplicates(group(kw)))\n",
    "answer_test = dictionarize(final_kw, test_ans_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_final = remove_duplicates(group(final_keywords))\n",
    "new_final = []\n",
    "for i in group_final:\n",
    "    if len(i) > 1:\n",
    "        for j in i:\n",
    "            for k in i:\n",
    "                if j != k and j in k:\n",
    "                    new_final.append(j)\n",
    "    else:\n",
    "        new_final.append(i[-1])\n",
    "# new_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "marks = 0\n",
    "for i in answer_key:\n",
    "    marks += len(answer_key[i])\n",
    "print(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(key, test):\n",
    "    vec_key_sent, vec_key_keys = vectorize_text(key)\n",
    "    vec_test_sent, vec_test_keys = vectorize_text(test)\n",
    "    sum = 0\n",
    "    sims = dict()\n",
    "    for i in range(len(vec_test_sent)):\n",
    "        sims[i] = []\n",
    "        for j in range(len(vec_key_sent)):\n",
    "            sim = cosine_similarity(vec_test_sent[i].reshape(\n",
    "                1, -1), vec_key_sent[j].reshape(1, -1))\n",
    "            if sim > 0.7:\n",
    "                sims[i].append(j)\n",
    "\n",
    "    count = 0\n",
    "    for keyidx in sims:\n",
    "        ans_kw = vec_test_keys[keyidx]\n",
    "        key_kw = []\n",
    "        checked = []\n",
    "        for i in sims[keyidx]:\n",
    "            key_kw.extend(vec_key_keys[i])\n",
    "\n",
    "        for akw in ans_kw:\n",
    "            max_sim = -1\n",
    "            max_kkw = []\n",
    "            for kkw in key_kw:\n",
    "                if kkw in checked:\n",
    "                    continue\n",
    "                sim = cosine_similarity(kkw, akw)[0][0]\n",
    "                if sim > max_sim:\n",
    "                    max_sim = sim\n",
    "                    max_akw = kkw\n",
    "            if sim > 0.9:\n",
    "                sum += 1\n",
    "#                 count += 1\n",
    "            else:\n",
    "                sum += max_sim\n",
    "#                 print(max_sim)\n",
    "#                 count += 1\n",
    "            checked.append(max_kkw)\n",
    "    return sum, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.76636290550232\n",
      "2/4\n"
     ]
    }
   ],
   "source": [
    "test_score, kw_match = score(answer_key, answer_test)\n",
    "print(test_score)\n",
    "test_score = test_score/marks*4.0\n",
    "# print(kw_match)\n",
    "if (test_score % 1) > 0.5:\n",
    "    rem = 1\n",
    "else:\n",
    "    rem = 0\n",
    "final_score = int(test_score)+rem\n",
    "print(final_score, \"/\", 4, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Machine learning is the scientific study of . F algorithms and statistical models that computer systems use to perform a specific task without using explicit instructions , relying on patterns and reference instead . It is seen as a subset of astiffrcial intelligence . Machine leading algorithms build a mathematical model lugt a sample data known as training . Machine learning algorithms ase used in a variety of applications . such as email filtering and computes vision where it is difficult or infeasible to develop a . Conventional algosiltm for effectively performing the task is # . B 3 #'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans_ml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
